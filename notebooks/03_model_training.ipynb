{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13512718",
   "metadata": {},
   "source": [
    "# ESA Project: Fake or Real: The Impostor Hunt in Texts\n",
    "\n",
    "This notebook is dedicated to **model training**.  \n",
    "It covers:\n",
    "\n",
    "- Load the pre-tokenized and saved PyTorch datasets (`tokenized_train.pt` and `tokenized_val.pt`) created in the data creation step.\n",
    "- Instantiate the pre-trained **DistilBertForSequenceClassification** model, configured for a binary classification task (Real or fake).\n",
    "- Define the **TrainingArguments** for the Hugging Face Trainer, including hyperparameters such as batch size, learning rate, number of epochs, and logging settings.\n",
    "- Implemente a function to compute and track key performance metrics on the validation set including **Accuracy, Precision, Recall and F1-score**.\n",
    "- Executing the training loop using the Hugging Face **Trainer** class which manages the entire process including checkpointing and model saving.\n",
    "- Save the best-performing model and its tokenizer to disk for later use in inference and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bd6ac",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa06c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/photoli93/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, EarlyStoppingCallback\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src folder to Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import config\n",
    "from preprocessing import TextPreprocessor, get_text_statistics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efcd26",
   "metadata": {},
   "source": [
    "# Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7a463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc9056",
   "metadata": {},
   "source": [
    "# Load tokenized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8229e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized training data from: /Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/data/processed/tokenized_train.pt\n",
      "Training dataset size: 212\n",
      "Validation dataset size: 41\n"
     ]
    }
   ],
   "source": [
    "TOKENIZED_TRAIN_PATH = config.PROCESSED_DATA_DIR / \"tokenized_train.pt\"\n",
    "TOKENIZED_VAL_PATH = config.PROCESSED_DATA_DIR / \"tokenized_val.pt\"\n",
    "\n",
    "OUTPUT_DIR = config.OUTPUT_DIR / \"distilbert_fake_or_real\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load tokenized datasets\n",
    "print(f\"Loading tokenized training data from: {TOKENIZED_TRAIN_PATH}\")\n",
    "try:\n",
    "    train_dataset = torch.load(TOKENIZED_TRAIN_PATH, weights_only=False)\n",
    "    val_dataset = torch.load(TOKENIZED_VAL_PATH, weights_only=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Tokenized data not found. Please ensure 03_dataset_creation.py (or your data creation notebook) was run successfully\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8614a",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0973e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: distilbert-base-uncased for Sequence Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model for sequence classification with 2 labels (Real/Fake)\n",
    "print(f\"Loading model: {config.TOKENIZER_NAME} for Sequence Classification\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    config.TOKENIZER_NAME, \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Might need the tokenizer later for prediction/evaluation\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fbd66",
   "metadata": {},
   "source": [
    "# Define parameters and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44483856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),          \n",
    "    num_train_epochs=10,                 # 10 epochs but with early stopping to avoid overfitting\n",
    "    per_device_train_batch_size=16,      \n",
    "    per_device_eval_batch_size=64,       \n",
    "    warmup_steps=500,                    # Nb of steps during which the learning rate increases learning\n",
    "    weight_decay=0.01,                   # Regularization term to reduce overfitting by penalizing large weights\n",
    "    logging_dir='./logs',                \n",
    "    logging_steps=50,                    # Log metrics every 50 training steps\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",               # Save model checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,         # Load the best model found during training\n",
    "    metric_for_best_model=\"eval_f1\",     # Metric to monitor for best model\n",
    "    fp16=True,                           # Use 16-bit precision for faster training\n",
    "    report_to=\"none\"                     # disables wandb, tensorboard, etc. (disabling sending logs online)\n",
    ")\n",
    "\n",
    "# Define Evaluation Metric\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    \n",
    "    # p is an object passed by the Hugging Face Trainer. It contains \"predictions\" and \"label_ids\"\n",
    "    def compute_metrics(p):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')\n",
    "        acc = accuracy_score(p.label_ids, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "except ImportError:\n",
    "    print(\"Warning: scikit-learn not found. Install with 'pip install scikit-learn' to use advanced metrics\")\n",
    "    def compute_metrics(p):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28240220",
   "metadata": {},
   "source": [
    "# Custom Loss function\n",
    "\n",
    "By exploding chunks, it has added unbalance in train dataset (138 for class 0 and 74 for class 1) so custom loss function has to be defined in order to add weights on classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb74d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    138\n",
      "1     74\n",
      "Name: count, dtype: int64\n",
      "tensor([0.3491, 0.6509])\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(config.PROCESSED_DATA_DIR / \"train_exploded.csv\")\n",
    "\n",
    "# Count of each class\n",
    "counts = train_df['label'].value_counts().sort_index()\n",
    "print(counts)\n",
    "\n",
    "# Dynamic class weights (inverse frequency)\n",
    "weights = 1.0 / counts\n",
    "weights = weights / weights.sum()  # Normalize\n",
    "weights = torch.tensor(weights.values, dtype=torch.float)\n",
    "print(weights)\n",
    "\n",
    "# Override the default loss\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    loss_fct = CrossEntropyLoss(weight=weights.to(logits.device))\n",
    "    loss = loss_fct(logits, labels)\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606c28b",
   "metadata": {},
   "source": [
    "# Custom subclass Trainer\n",
    "\n",
    "Since in Hugging Face Transformers, the **Trainer** class does not have a `compute_loss` argument in its constructor, I created a subclass called **WeightedTrainer** to use the custom `compute_loss` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2575ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Define class weights\n",
    "        weights = torch.tensor([1.0, 138/74]).to(logits.device)\n",
    "        loss_fct = CrossEntropyLoss(weight=weights)\n",
    "        \n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf8e25",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1da8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd63464baed4163a5df29ddc3275c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad283d37d1a54df199344dff0e102a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6819775700569153, 'eval_accuracy': 0.4634146341463415, 'eval_f1': 0.6333333333333333, 'eval_precision': 0.4634146341463415, 'eval_recall': 1.0, 'eval_runtime': 2.4839, 'eval_samples_per_second': 16.506, 'eval_steps_per_second': 0.403, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a74ce2ad0a4582a8535495f9b0953c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6654515862464905, 'eval_accuracy': 0.6585365853658537, 'eval_f1': 0.7307692307692307, 'eval_precision': 0.5757575757575758, 'eval_recall': 1.0, 'eval_runtime': 2.3248, 'eval_samples_per_second': 17.636, 'eval_steps_per_second': 0.43, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9782f694a4475884c4fc732c5e801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6270184516906738, 'eval_accuracy': 0.7560975609756098, 'eval_f1': 0.7916666666666666, 'eval_precision': 0.6551724137931034, 'eval_recall': 1.0, 'eval_runtime': 2.2782, 'eval_samples_per_second': 17.996, 'eval_steps_per_second': 0.439, 'epoch': 3.0}\n",
      "{'loss': 0.6564, 'grad_norm': 1.9313827753067017, 'learning_rate': 5e-06, 'epoch': 3.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9ff3569b1a4c668046fca55ea82067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5431719422340393, 'eval_accuracy': 0.8292682926829268, 'eval_f1': 0.8444444444444444, 'eval_precision': 0.7307692307692307, 'eval_recall': 1.0, 'eval_runtime': 2.2877, 'eval_samples_per_second': 17.922, 'eval_steps_per_second': 0.437, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397b03bfe97f45b79efb64562ccb6965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4610339403152466, 'eval_accuracy': 0.7560975609756098, 'eval_f1': 0.7916666666666666, 'eval_precision': 0.6551724137931034, 'eval_recall': 1.0, 'eval_runtime': 2.3416, 'eval_samples_per_second': 17.509, 'eval_steps_per_second': 0.427, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a46902d6074a0488d1c88d35a15d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35856807231903076, 'eval_accuracy': 0.8780487804878049, 'eval_f1': 0.8717948717948718, 'eval_precision': 0.85, 'eval_recall': 0.8947368421052632, 'eval_runtime': 2.3095, 'eval_samples_per_second': 17.753, 'eval_steps_per_second': 0.433, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01575291ca81490183ac4e7bea62bb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3103157579898834, 'eval_accuracy': 0.8292682926829268, 'eval_f1': 0.8292682926829268, 'eval_precision': 0.7727272727272727, 'eval_recall': 0.8947368421052632, 'eval_runtime': 2.2457, 'eval_samples_per_second': 18.257, 'eval_steps_per_second': 0.445, 'epoch': 7.0}\n",
      "{'loss': 0.3926, 'grad_norm': 4.192562103271484, 'learning_rate': 9.900000000000002e-06, 'epoch': 7.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3248d56dff1c4a53a7c46bc5928d3f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2897054851055145, 'eval_accuracy': 0.8292682926829268, 'eval_f1': 0.8292682926829268, 'eval_precision': 0.7727272727272727, 'eval_recall': 0.8947368421052632, 'eval_runtime': 2.3161, 'eval_samples_per_second': 17.702, 'eval_steps_per_second': 0.432, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e59886416d4f58a6681f14c1ff7f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34315812587738037, 'eval_accuracy': 0.8780487804878049, 'eval_f1': 0.8717948717948718, 'eval_precision': 0.85, 'eval_recall': 0.8947368421052632, 'eval_runtime': 2.3241, 'eval_samples_per_second': 17.641, 'eval_steps_per_second': 0.43, 'epoch': 9.0}\n",
      "{'train_runtime': 203.3584, 'train_samples_per_second': 20.85, 'train_steps_per_second': 1.377, 'train_loss': 0.4672575488923088, 'epoch': 9.0}\n",
      "Training complete!\n",
      "Final model and tokenizer saved to: /Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/results/distilbert_fake_or_real/final_model\n",
      "\n",
      "Final evaluation on validation set:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9571a3d011ec4bf48956d1b711bc3f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35856807231903076, 'eval_accuracy': 0.8780487804878049, 'eval_f1': 0.8717948717948718, 'eval_precision': 0.85, 'eval_recall': 0.8947368421052632, 'eval_runtime': 2.1548, 'eval_samples_per_second': 19.027, 'eval_steps_per_second': 0.464, 'epoch': 9.0}\n",
      "\n",
      "Model Training Complete\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WeightedTrainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,                                                  # The instantiated Transformers model to be trained\n",
    "    args=training_args,                                           # Training arguments, defined above\n",
    "    train_dataset=train_dataset,                                  # Training dataset\n",
    "    eval_dataset=val_dataset,                                     # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,                              # Function to compute metrics\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # stop if no improvement in 3 epochs\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"\\nStarting training\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save Final Model\n",
    "final_model_path = OUTPUT_DIR / \"final_model\"\n",
    "trainer.save_model(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "print(f\"Final model and tokenizer saved to: {final_model_path}\")\n",
    "\n",
    "# Final Evaluation\n",
    "print(\"\\nFinal evaluation on validation set:\")\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "print(\"\\nModel Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3dcc7",
   "metadata": {},
   "source": [
    "# End of model training notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa_fake_or_real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
