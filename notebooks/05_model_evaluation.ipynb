{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53dfb21",
   "metadata": {},
   "source": [
    "# ESA Project: Fake or Real: The Impostor Hunt in Texts\n",
    "\n",
    "This notebook is dedicated to **model evaluation**.  \n",
    "It covers:\n",
    "\n",
    "- Load the saved, best-performing `DistilBERT` model from the training phase and the tokenized validation dataset\n",
    "- Run inference on the validation set to obtain the model's predicted probability of the \"Real\" class (`label=1`) for every individual chunk\n",
    "- Apply the Realness score comparison strategy to combine chunk-level predictions: the average \"Real\" probability of all chunks is calculated for `file1` and `file2`\n",
    "- Calculate the final Accuracy of the `DistilBERT` model on the original text pair classification task\n",
    "- Compare the `DistilBERT` model's final accuracy against the baseline model's performance to validate the use of the complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c1162",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9f3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add the src folder to Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f66d49",
   "metadata": {},
   "source": [
    "# Custom classes from the model's training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c506a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define the custom dataset class\n",
    "class TextPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Define the custom trainer class with weighted loss\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Define class weights\n",
    "        # Had 0 - 138 and 1 - 74\n",
    "        weights = torch.tensor([1.0, 138/74]).to(logits.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        \n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    # Note: 'binary' average is used because the task is binary classification (0 or 1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Detect device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "MODEL_PATH = config.OUTPUT_DIR / \"distilbert_fake_or_real\" / \"final_model\"\n",
    "TOKENIZED_TEST_PATH = config.PROCESSED_DATA_DIR / \"tokenized_test.pt\"\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6396b",
   "metadata": {},
   "source": [
    "# Load data and final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af246007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 2\n",
      "torch.Size([2608])\n",
      "Model and tokenizer loaded from: /Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/results/distilbert_fake_or_real/final_model\n"
     ]
    }
   ],
   "source": [
    "# Load tokenized test dataset\n",
    "try:\n",
    "    # Note: You must have a tokenized_test.pt file ready\n",
    "    test_dataset = torch.load(TOKENIZED_TEST_PATH, weights_only=False)\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Tokenized test data not found at {TOKENIZED_TEST_PATH}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Add labels if needed\n",
    "if \"labels\" not in test_dataset:\n",
    "    n = test_dataset[\"input_ids\"].size(0)\n",
    "    test_dataset[\"labels\"] = torch.zeros(n, dtype=torch.long)\n",
    "\n",
    "# Check\n",
    "print(test_dataset[\"labels\"].shape)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "hf_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_dataset[\"input_ids\"].tolist(),\n",
    "    \"attention_mask\": test_dataset[\"attention_mask\"].tolist(),\n",
    "    \"labels\": test_dataset[\"labels\"].tolist()\n",
    "})\n",
    "\n",
    "# Load the final model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "print(f\"Model and tokenizer loaded from: {MODEL_PATH}\")\n",
    "\n",
    "# Define minimal training arguments for evaluation\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=str(config.OUTPUT_DIR / \"evaluation\"),\n",
    "    per_device_eval_batch_size=64,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Instantiate Trainer for evaluation\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=hf_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf322d",
   "metadata": {},
   "source": [
    "# Model prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65b02f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Starting final prediction on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/.conda/esa_fake_or_real/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ea68fdbb7f48c4a0fda6b49eac1ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics\n",
      "PredictionOutput(predictions=array([[-0.8699257 ,  1.1607869 ],\n",
      "       [ 0.89736944, -0.85450935],\n",
      "       [-0.76462257,  1.1284206 ],\n",
      "       ...,\n",
      "       [ 0.63882816, -0.4171373 ],\n",
      "       [-0.82367814,  1.222086  ],\n",
      "       [ 0.09846318,  0.1728112 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 1.1365916728973389, 'test_model_preparation_time': 0.0007, 'test_accuracy': 0.42829754601226994, 'test_f1': 0.0, 'test_precision': 0.0, 'test_recall': 0.0, 'test_runtime': 124.2052, 'test_samples_per_second': 20.998, 'test_steps_per_second': 0.33})\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/.conda/esa_fake_or_real/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 80)\n",
    "print(\"Starting final prediction on test set\")\n",
    "predictions = trainer.predict(hf_dataset)\n",
    "\n",
    "print(\"\\nTest Set Metrics\")\n",
    "print(predictions)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Logits from Hugging Face trainer.predict\n",
    "logits = predictions.predictions\n",
    "# Convert logits to probabilities\n",
    "probs = softmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a7088",
   "metadata": {},
   "source": [
    "# Reconstruction of the test dataset (unchunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297d173e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>file1_text</th>\n",
       "      <th>file2_text</th>\n",
       "      <th>file1_text_cleaned</th>\n",
       "      <th>file2_text_cleaned</th>\n",
       "      <th>file1_text_cleaned_tokens</th>\n",
       "      <th>file1_text_cleaned_num_tokens</th>\n",
       "      <th>file2_text_cleaned_tokens</th>\n",
       "      <th>file2_text_cleaned_num_tokens</th>\n",
       "      <th>file1_text_cleaned_chunks</th>\n",
       "      <th>file1_text_cleaned_num_chunks</th>\n",
       "      <th>file2_text_cleaned_chunks</th>\n",
       "      <th>file2_text_cleaned_num_chunks</th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>source</th>\n",
       "      <th>prob_file1_real</th>\n",
       "      <th>prob_file2_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The brightest blue star found within an area c...</td>\n",
       "      <td>GEBURTSiostream\\nThe primary standout object o...</td>\n",
       "      <td>bright blue star find within area call h ii re...</td>\n",
       "      <td>geburtsiostream primary standout object observ...</td>\n",
       "      <td>['bright', 'blue', 'star', 'find', 'within', '...</td>\n",
       "      <td>100</td>\n",
       "      <td>['ge', '##bu', '##rts', '##ios', '##tream', 'p...</td>\n",
       "      <td>198</td>\n",
       "      <td>['bright blue star find within area call h ii ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['geburtsiostream primary standout object obse...</td>\n",
       "      <td>1</td>\n",
       "      <td>bright blue star find within area call h ii re...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0.116016</td>\n",
       "      <td>0.883984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The frontiers of space medicine are exploding ...</td>\n",
       "      <td>In recent decades, scientists have focused on ...</td>\n",
       "      <td>frontier space medicine explode exciting disco...</td>\n",
       "      <td>recent decade scientist focus understand super...</td>\n",
       "      <td>['frontier', 'space', 'medicine', 'explode', '...</td>\n",
       "      <td>214</td>\n",
       "      <td>['recent', 'decade', 'scientist', 'focus', 'un...</td>\n",
       "      <td>153</td>\n",
       "      <td>['frontier space medicine explode exciting dis...</td>\n",
       "      <td>1</td>\n",
       "      <td>['recent decade scientist focus understand sup...</td>\n",
       "      <td>1</td>\n",
       "      <td>frontier space medicine explode exciting disco...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0.852190</td>\n",
       "      <td>0.147810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The second-generation Variable Light Telescope...</td>\n",
       "      <td>**A Giant Leap: The DSM Takes Flight!**\\n\\nLik...</td>\n",
       "      <td>second generation variable light telescope vlt...</td>\n",
       "      <td>giant leap dsm take flight like giant star rea...</td>\n",
       "      <td>['second', 'generation', 'variable', 'light', ...</td>\n",
       "      <td>175</td>\n",
       "      <td>['giant', 'leap', 'ds', '##m', 'take', 'flight...</td>\n",
       "      <td>214</td>\n",
       "      <td>['second generation variable light telescope v...</td>\n",
       "      <td>1</td>\n",
       "      <td>['giant leap dsm take flight like giant star r...</td>\n",
       "      <td>1</td>\n",
       "      <td>second generation variable light telescope vlt...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0.130898</td>\n",
       "      <td>0.869102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>This study examined whether articles published...</td>\n",
       "      <td>underground\\nWe analyzed how often articles pu...</td>\n",
       "      <td>study examine whether article publish arxiv re...</td>\n",
       "      <td>underground analyze often article publish arxi...</td>\n",
       "      <td>['study', 'examine', 'whether', 'article', 'pu...</td>\n",
       "      <td>126</td>\n",
       "      <td>['underground', 'analyze', 'often', 'article',...</td>\n",
       "      <td>135</td>\n",
       "      <td>['study examine whether article publish arxiv ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['underground analyze often article publish ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>study examine whether article publish arxiv re...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0.241505</td>\n",
       "      <td>0.758495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The special abilities of ALMA were highlighted...</td>\n",
       "      <td>The distinctive abilities of ALMA were highlig...</td>\n",
       "      <td>special ability alma highlight improve underst...</td>\n",
       "      <td>distinctive ability alma highlight enhance und...</td>\n",
       "      <td>['special', 'ability', 'alma', 'highlight', 'i...</td>\n",
       "      <td>134</td>\n",
       "      <td>['distinctive', 'ability', 'alma', 'highlight'...</td>\n",
       "      <td>144</td>\n",
       "      <td>['special ability alma highlight improve under...</td>\n",
       "      <td>1</td>\n",
       "      <td>['distinctive ability alma highlight enhance u...</td>\n",
       "      <td>1</td>\n",
       "      <td>special ability alma highlight improve underst...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0.122651</td>\n",
       "      <td>0.877349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                                         file1_text  \\\n",
       "0        0  The brightest blue star found within an area c...   \n",
       "1        1  The frontiers of space medicine are exploding ...   \n",
       "2        2  The second-generation Variable Light Telescope...   \n",
       "3        3  This study examined whether articles published...   \n",
       "4        4  The special abilities of ALMA were highlighted...   \n",
       "\n",
       "                                          file2_text  \\\n",
       "0  GEBURTSiostream\\nThe primary standout object o...   \n",
       "1  In recent decades, scientists have focused on ...   \n",
       "2  **A Giant Leap: The DSM Takes Flight!**\\n\\nLik...   \n",
       "3  underground\\nWe analyzed how often articles pu...   \n",
       "4  The distinctive abilities of ALMA were highlig...   \n",
       "\n",
       "                                  file1_text_cleaned  \\\n",
       "0  bright blue star find within area call h ii re...   \n",
       "1  frontier space medicine explode exciting disco...   \n",
       "2  second generation variable light telescope vlt...   \n",
       "3  study examine whether article publish arxiv re...   \n",
       "4  special ability alma highlight improve underst...   \n",
       "\n",
       "                                  file2_text_cleaned  \\\n",
       "0  geburtsiostream primary standout object observ...   \n",
       "1  recent decade scientist focus understand super...   \n",
       "2  giant leap dsm take flight like giant star rea...   \n",
       "3  underground analyze often article publish arxi...   \n",
       "4  distinctive ability alma highlight enhance und...   \n",
       "\n",
       "                           file1_text_cleaned_tokens  \\\n",
       "0  ['bright', 'blue', 'star', 'find', 'within', '...   \n",
       "1  ['frontier', 'space', 'medicine', 'explode', '...   \n",
       "2  ['second', 'generation', 'variable', 'light', ...   \n",
       "3  ['study', 'examine', 'whether', 'article', 'pu...   \n",
       "4  ['special', 'ability', 'alma', 'highlight', 'i...   \n",
       "\n",
       "   file1_text_cleaned_num_tokens  \\\n",
       "0                            100   \n",
       "1                            214   \n",
       "2                            175   \n",
       "3                            126   \n",
       "4                            134   \n",
       "\n",
       "                           file2_text_cleaned_tokens  \\\n",
       "0  ['ge', '##bu', '##rts', '##ios', '##tream', 'p...   \n",
       "1  ['recent', 'decade', 'scientist', 'focus', 'un...   \n",
       "2  ['giant', 'leap', 'ds', '##m', 'take', 'flight...   \n",
       "3  ['underground', 'analyze', 'often', 'article',...   \n",
       "4  ['distinctive', 'ability', 'alma', 'highlight'...   \n",
       "\n",
       "   file2_text_cleaned_num_tokens  \\\n",
       "0                            198   \n",
       "1                            153   \n",
       "2                            214   \n",
       "3                            135   \n",
       "4                            144   \n",
       "\n",
       "                           file1_text_cleaned_chunks  \\\n",
       "0  ['bright blue star find within area call h ii ...   \n",
       "1  ['frontier space medicine explode exciting dis...   \n",
       "2  ['second generation variable light telescope v...   \n",
       "3  ['study examine whether article publish arxiv ...   \n",
       "4  ['special ability alma highlight improve under...   \n",
       "\n",
       "   file1_text_cleaned_num_chunks  \\\n",
       "0                              1   \n",
       "1                              1   \n",
       "2                              1   \n",
       "3                              1   \n",
       "4                              1   \n",
       "\n",
       "                           file2_text_cleaned_chunks  \\\n",
       "0  ['geburtsiostream primary standout object obse...   \n",
       "1  ['recent decade scientist focus understand sup...   \n",
       "2  ['giant leap dsm take flight like giant star r...   \n",
       "3  ['underground analyze often article publish ar...   \n",
       "4  ['distinctive ability alma highlight enhance u...   \n",
       "\n",
       "   file2_text_cleaned_num_chunks  \\\n",
       "0                              1   \n",
       "1                              1   \n",
       "2                              1   \n",
       "3                              1   \n",
       "4                              1   \n",
       "\n",
       "                                          text_chunk source  prob_file1_real  \\\n",
       "0  bright blue star find within area call h ii re...  file1         0.116016   \n",
       "1  frontier space medicine explode exciting disco...  file1         0.852190   \n",
       "2  second generation variable light telescope vlt...  file1         0.130898   \n",
       "3  study examine whether article publish arxiv re...  file1         0.241505   \n",
       "4  special ability alma highlight improve underst...  file1         0.122651   \n",
       "\n",
       "   prob_file2_real  \n",
       "0         0.883984  \n",
       "1         0.147810  \n",
       "2         0.869102  \n",
       "3         0.758495  \n",
       "4         0.877349  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  real_text_id\n",
       "0   0             2\n",
       "1   1             2\n",
       "2   2             1\n",
       "3   3             2\n",
       "4   4             2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df_exploded = pd.read_csv(\"../data/processed/test_exploded.csv\")\n",
    "\n",
    "test_df_exploded[\"prob_file1_real\"] = probs[:,0]\n",
    "test_df_exploded[\"prob_file2_real\"] = probs[:,1]\n",
    "display(test_df_exploded.head())\n",
    "\n",
    "# Sum probabilities over chunks per pair\n",
    "agg_probs = test_df_exploded.groupby(\"pair_id\")[[\"prob_file1_real\", \"prob_file2_real\"]].sum().reset_index()\n",
    "# Decide which file is real\n",
    "agg_probs[\"real_text_id\"] = np.where(\n",
    "    agg_probs[\"prob_file1_real\"] > agg_probs[\"prob_file2_real\"], 1, 2\n",
    ")\n",
    "\n",
    "agg_probs = agg_probs[[\"pair_id\", \"real_text_id\"]].rename(columns={\"pair_id\": \"id\"})\n",
    "display(agg_probs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290ebee",
   "metadata": {},
   "source": [
    "# Export final df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94473368",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_probs.to_csv(\"../data/processed/final_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f56d7",
   "metadata": {},
   "source": [
    "# End of model evaluation notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa_fake_or_real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
