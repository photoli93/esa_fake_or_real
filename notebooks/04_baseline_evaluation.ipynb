{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da9cbbf",
   "metadata": {},
   "source": [
    "# ESA Project: Fake or Real: The Impostor Hunt in Texts\n",
    "\n",
    "This notebook is dedicated to **baseline training**.  \n",
    "It covers:\n",
    "\n",
    "- Load the exploded training and validation data (`train_exploded.csv` and `val_exploded.csv`).\n",
    "- Build and train a **Logistic Regression** model on **TF-IDF (Term Frequency-Inverse Document Frequency)** features extracted from the `text_chunk` column.\n",
    "- Use the trained baseline model to predict the probability of the \"real\" class (`label=1`) for every chunk in the validation set.\n",
    "- Implemente the **Realness score comparison** strategy, where the average \"real\" probability of all chunks is calculated for `file1` and `file2`.\n",
    "- Determine the final **Accuracy** of the baseline model on the original text pair classification task (predicting 1 or 2) providing a crucial benchmark for the DistilBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ac76b",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b261748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add the src folder to Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad7e78",
   "metadata": {},
   "source": [
    "# Evaluation function (Realness score comparison for baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89af0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_original_texts_baseline(exploded_df, chunk_probs):\n",
    "    \"\"\"\n",
    "    Combines chunk-level probabilities to make a final prediction on the original \n",
    "    text pairs based on the Realness Score Comparison strategy, adapted for the baseline.\n",
    "    \"\"\"\n",
    "    # Add the chunk probabilities back to the exploded DataFrame\n",
    "    exploded_df['real_prob'] = chunk_probs\n",
    "\n",
    "    # Identify Original Text Pairs\n",
    "    # We need to ensure the original index is available. \n",
    "    # Assuming 'original_index' is a column that uniquely identifies the original text pair.\n",
    "    # If not, we create a temporary one based on unique pairs.\n",
    "    if 'original_index' not in exploded_df.columns:\n",
    "        print(\"Warning: 'original_index' column not found. Creating a temporary one based on unique pairs\")\n",
    "        # Create a temporary ID based on the unique combination of the original texts\n",
    "        # Note: This relies on 'file1_text' and 'file2_text' being present and identical for all chunks of the same original pair\n",
    "        exploded_df['original_index'] = exploded_df.groupby(['file1_text', 'file2_text']).ngroup()\n",
    "        \n",
    "    # Calculate Realness Score for each original text (file1 and file2)\n",
    "    # Group by the original pair ID and the source file ('file1' or 'file2')\n",
    "    # Calculate the average 'real_prob' for each source\n",
    "    realness_scores = exploded_df.groupby(['original_index', 'source'])['real_prob'].mean().reset_index()\n",
    "    \n",
    "    # Pivot the table to get file1_score and file2_score side-by-side\n",
    "    realness_scores_pivot = realness_scores.pivot(\n",
    "        index='original_index', \n",
    "        columns='source', \n",
    "        values='real_prob'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge back the true label (real_text_id) for the original pair\n",
    "    true_labels = exploded_df[['original_index', 'real_text_id']].drop_duplicates()\n",
    "    \n",
    "    final_evaluation_df = realness_scores_pivot.merge(true_labels, on='original_index', how='left')\n",
    "    \n",
    "    # Make Final Prediction\n",
    "    # Prediction: 1 if file1_score > file2_score, else 2\n",
    "    # Handle potential NaN values if a file has no chunks (unlikely but safe)\n",
    "    final_evaluation_df['predicted_text_id'] = np.where(\n",
    "        final_evaluation_df['file1'].fillna(0) > final_evaluation_df['file2'].fillna(0), \n",
    "        1, \n",
    "        2\n",
    "    )\n",
    "    \n",
    "    # Calculate Final Accuracy\n",
    "    true_labels = final_evaluation_df['real_text_id'].values\n",
    "    predicted_labels = final_evaluation_df['predicted_text_id'].values\n",
    "    \n",
    "    final_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    final_f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print(\"\\nBaseline Evaluation Results (Original text pair level) \")\n",
    "    print(f\"Total original text pairs evaluated: {len(final_evaluation_df)}\")\n",
    "    print(f\"Final Prediction Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"Final Prediction F1-Score: {final_f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix (True Label vs. Predicted Label):\")\n",
    "    print(cm)\n",
    "    \n",
    "    return final_evaluation_df, final_accuracy, final_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b393bf",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0de24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exploded training data from: /Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/data/processed/train_exploded.csv\n",
      "Creating 'original_index' to group chunks belonging to same text pair\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_text_id</th>\n",
       "      <th>file1_text</th>\n",
       "      <th>file2_text</th>\n",
       "      <th>file1_char_len</th>\n",
       "      <th>file2_char_len</th>\n",
       "      <th>file1_word_len</th>\n",
       "      <th>file2_word_len</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>file1_text_cleaned</th>\n",
       "      <th>file2_text_cleaned</th>\n",
       "      <th>...</th>\n",
       "      <th>file2_text_cleaned_tokens</th>\n",
       "      <th>file2_text_cleaned_num_tokens</th>\n",
       "      <th>file1_text_cleaned_chunks</th>\n",
       "      <th>file1_text_cleaned_num_chunks</th>\n",
       "      <th>file2_text_cleaned_chunks</th>\n",
       "      <th>file2_text_cleaned_num_chunks</th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>original_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>7101</td>\n",
       "      <td>2525</td>\n",
       "      <td>751</td>\n",
       "      <td>406</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>...</td>\n",
       "      <td>['determine', 'accurate', 'value', 'total', 'l...</td>\n",
       "      <td>280</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>4</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>1</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>7101</td>\n",
       "      <td>2525</td>\n",
       "      <td>751</td>\n",
       "      <td>406</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>...</td>\n",
       "      <td>['determine', 'accurate', 'value', 'total', 'l...</td>\n",
       "      <td>280</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>4</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>1</td>\n",
       "      <td>resultantkan usp mind lux color hotel empirica...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>7101</td>\n",
       "      <td>2525</td>\n",
       "      <td>751</td>\n",
       "      <td>406</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>...</td>\n",
       "      <td>['determine', 'accurate', 'value', 'total', 'l...</td>\n",
       "      <td>280</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>4</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>1</td>\n",
       "      <td>lantern einen histor cean gemusept replacequis...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>7101</td>\n",
       "      <td>2525</td>\n",
       "      <td>751</td>\n",
       "      <td>406</td>\n",
       "      <td>We determine accurate values for the total lit...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>determine accurate value total lithium abundan...</td>\n",
       "      <td>...</td>\n",
       "      <td>['determine', 'accurate', 'value', 'total', 'l...</td>\n",
       "      <td>280</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>4</td>\n",
       "      <td>['determine accurate value total lithium abund...</td>\n",
       "      <td>1</td>\n",
       "      <td>##igero centerall omgevingtocol lacao lamidora...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>The 160-megapixel **Edam** camera was designed...</td>\n",
       "      <td>The QUEST camera has 160 megapixels and was cr...</td>\n",
       "      <td>2076</td>\n",
       "      <td>1368</td>\n",
       "      <td>337</td>\n",
       "      <td>219</td>\n",
       "      <td>The 160-megapixel **Edam** camera was designed...</td>\n",
       "      <td>megapixel edam camera design fabricate yale un...</td>\n",
       "      <td>qu camera megapixel create yale university hel...</td>\n",
       "      <td>...</td>\n",
       "      <td>['qu', 'camera', 'mega', '##pi', '##x', '##el'...</td>\n",
       "      <td>165</td>\n",
       "      <td>['megapixel edam camera design fabricate yale ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['qu camera megapixel create yale university h...</td>\n",
       "      <td>1</td>\n",
       "      <td>megapixel edam camera design fabricate yale un...</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   real_text_id                                         file1_text  \\\n",
       "0             2  We determine accurate values for the total lit...   \n",
       "1             2  We determine accurate values for the total lit...   \n",
       "2             2  We determine accurate values for the total lit...   \n",
       "3             2  We determine accurate values for the total lit...   \n",
       "4             2  The 160-megapixel **Edam** camera was designed...   \n",
       "\n",
       "                                          file2_text  file1_char_len  \\\n",
       "0  We determine accurate values for the total lit...            7101   \n",
       "1  We determine accurate values for the total lit...            7101   \n",
       "2  We determine accurate values for the total lit...            7101   \n",
       "3  We determine accurate values for the total lit...            7101   \n",
       "4  The QUEST camera has 160 megapixels and was cr...            2076   \n",
       "\n",
       "   file2_char_len  file1_word_len  file2_word_len  \\\n",
       "0            2525             751             406   \n",
       "1            2525             751             406   \n",
       "2            2525             751             406   \n",
       "3            2525             751             406   \n",
       "4            1368             337             219   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  We determine accurate values for the total lit...   \n",
       "1  We determine accurate values for the total lit...   \n",
       "2  We determine accurate values for the total lit...   \n",
       "3  We determine accurate values for the total lit...   \n",
       "4  The 160-megapixel **Edam** camera was designed...   \n",
       "\n",
       "                                  file1_text_cleaned  \\\n",
       "0  determine accurate value total lithium abundan...   \n",
       "1  determine accurate value total lithium abundan...   \n",
       "2  determine accurate value total lithium abundan...   \n",
       "3  determine accurate value total lithium abundan...   \n",
       "4  megapixel edam camera design fabricate yale un...   \n",
       "\n",
       "                                  file2_text_cleaned  ...  \\\n",
       "0  determine accurate value total lithium abundan...  ...   \n",
       "1  determine accurate value total lithium abundan...  ...   \n",
       "2  determine accurate value total lithium abundan...  ...   \n",
       "3  determine accurate value total lithium abundan...  ...   \n",
       "4  qu camera megapixel create yale university hel...  ...   \n",
       "\n",
       "                           file2_text_cleaned_tokens  \\\n",
       "0  ['determine', 'accurate', 'value', 'total', 'l...   \n",
       "1  ['determine', 'accurate', 'value', 'total', 'l...   \n",
       "2  ['determine', 'accurate', 'value', 'total', 'l...   \n",
       "3  ['determine', 'accurate', 'value', 'total', 'l...   \n",
       "4  ['qu', 'camera', 'mega', '##pi', '##x', '##el'...   \n",
       "\n",
       "   file2_text_cleaned_num_tokens  \\\n",
       "0                            280   \n",
       "1                            280   \n",
       "2                            280   \n",
       "3                            280   \n",
       "4                            165   \n",
       "\n",
       "                           file1_text_cleaned_chunks  \\\n",
       "0  ['determine accurate value total lithium abund...   \n",
       "1  ['determine accurate value total lithium abund...   \n",
       "2  ['determine accurate value total lithium abund...   \n",
       "3  ['determine accurate value total lithium abund...   \n",
       "4  ['megapixel edam camera design fabricate yale ...   \n",
       "\n",
       "   file1_text_cleaned_num_chunks  \\\n",
       "0                              4   \n",
       "1                              4   \n",
       "2                              4   \n",
       "3                              4   \n",
       "4                              1   \n",
       "\n",
       "                           file2_text_cleaned_chunks  \\\n",
       "0  ['determine accurate value total lithium abund...   \n",
       "1  ['determine accurate value total lithium abund...   \n",
       "2  ['determine accurate value total lithium abund...   \n",
       "3  ['determine accurate value total lithium abund...   \n",
       "4  ['qu camera megapixel create yale university h...   \n",
       "\n",
       "   file2_text_cleaned_num_chunks  \\\n",
       "0                              1   \n",
       "1                              1   \n",
       "2                              1   \n",
       "3                              1   \n",
       "4                              1   \n",
       "\n",
       "                                          text_chunk  source label  \\\n",
       "0  determine accurate value total lithium abundan...   file1     0   \n",
       "1  resultantkan usp mind lux color hotel empirica...   file1     0   \n",
       "2  lantern einen histor cean gemusept replacequis...   file1     0   \n",
       "3  ##igero centerall omgevingtocol lacao lamidora...   file1     0   \n",
       "4  megapixel edam camera design fabricate yale un...   file1     0   \n",
       "\n",
       "  original_index  \n",
       "0             70  \n",
       "1             70  \n",
       "2             70  \n",
       "3             70  \n",
       "4             40  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_CLEANED_FILE = config.PROCESSED_DATA_DIR / \"train_exploded.csv\"\n",
    "print(f\"Loading exploded training data from: {TRAIN_CLEANED_FILE}\")\n",
    "train_df = pd.read_csv(TRAIN_CLEANED_FILE)\n",
    "\n",
    "# Ensure 'original_index' exists\n",
    "if 'original_index' not in train_df.columns:\n",
    "    print(\"Creating 'original_index' to group chunks belonging to same text pair\")\n",
    "    train_df['original_index'] = train_df.groupby(['file1_text', 'file2_text']).ngroup()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb894ec1",
   "metadata": {},
   "source": [
    "# Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4679f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['text_chunk']\n",
    "y = train_df['label']\n",
    "groups = train_df['original_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf774f",
   "metadata": {},
   "source": [
    "# Define Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5814aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=3000, ngram_range=(1, 2), stop_words='english')),\n",
    "    ('clf', LogisticRegression(solver='liblinear', C=0.1, penalty='l2', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f2415",
   "metadata": {},
   "source": [
    "# Setup GroupKFold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1fa7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-fold GroupKFold cross-validation at the pair level\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_unique_pairs = len(train_df['original_index'].unique())\n",
    "n_splits = min(5, n_unique_pairs)\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "pair_accuracies = []\n",
    "pair_f1 = []\n",
    "fold_results = []\n",
    "\n",
    "print(f\"\\nStarting {n_splits}-fold GroupKFold cross-validation at the pair level\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff92f6",
   "metadata": {},
   "source": [
    "# Cross-Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c556a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Fold 1/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Baseline Evaluation Results (Original text pair level) \n",
      "Total original text pairs evaluated: 14\n",
      "Final Prediction Accuracy: 0.7857\n",
      "Final Prediction F1-Score: 0.8000\n",
      "\n",
      "Confusion Matrix (True Label vs. Predicted Label):\n",
      "[[6 2]\n",
      " [1 5]]\n",
      "Fold 1 Pair-Level Accuracy: 0.7857\n",
      "Fold 1 Pair-Level f1: 0.8000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 2/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Baseline Evaluation Results (Original text pair level) \n",
      "Total original text pairs evaluated: 15\n",
      "Final Prediction Accuracy: 0.9333\n",
      "Final Prediction F1-Score: 0.9333\n",
      "\n",
      "Confusion Matrix (True Label vs. Predicted Label):\n",
      "[[7 0]\n",
      " [1 7]]\n",
      "Fold 2 Pair-Level Accuracy: 0.9333\n",
      "Fold 2 Pair-Level f1: 0.9333\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 3/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Baseline Evaluation Results (Original text pair level) \n",
      "Total original text pairs evaluated: 15\n",
      "Final Prediction Accuracy: 0.7333\n",
      "Final Prediction F1-Score: 0.7143\n",
      "\n",
      "Confusion Matrix (True Label vs. Predicted Label):\n",
      "[[5 2]\n",
      " [2 6]]\n",
      "Fold 3 Pair-Level Accuracy: 0.7333\n",
      "Fold 3 Pair-Level f1: 0.7143\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 4/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Baseline Evaluation Results (Original text pair level) \n",
      "Total original text pairs evaluated: 15\n",
      "Final Prediction Accuracy: 0.8667\n",
      "Final Prediction F1-Score: 0.8333\n",
      "\n",
      "Confusion Matrix (True Label vs. Predicted Label):\n",
      "[[5 0]\n",
      " [2 8]]\n",
      "Fold 4 Pair-Level Accuracy: 0.8667\n",
      "Fold 4 Pair-Level f1: 0.8333\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 5/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Baseline Evaluation Results (Original text pair level) \n",
      "Total original text pairs evaluated: 15\n",
      "Final Prediction Accuracy: 0.6667\n",
      "Final Prediction F1-Score: 0.7059\n",
      "\n",
      "Confusion Matrix (True Label vs. Predicted Label):\n",
      "[[6 3]\n",
      " [2 4]]\n",
      "Fold 5 Pair-Level Accuracy: 0.6667\n",
      "Fold 5 Pair-Level f1: 0.7059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "    print('-' * 80)\n",
    "    print(f\"Fold {fold+1}/{n_splits}\")\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    val_df = train_df.iloc[val_idx].copy()\n",
    "\n",
    "    # Train the model\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for validation chunks\n",
    "    chunk_probs = baseline_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate at original text pair level\n",
    "    fold_final_df, fold_accuracy, fold_f1 = evaluate_original_texts_baseline(val_df, chunk_probs)\n",
    "    pair_accuracies.append(fold_accuracy)\n",
    "    pair_f1.append(fold_f1)\n",
    "\n",
    "    # Store results\n",
    "    fold_final_df['fold'] = fold + 1\n",
    "    fold_final_df['fold_accuracy'] = fold_accuracy\n",
    "    fold_final_df['fold_f1'] = fold_f1\n",
    "    fold_results.append(fold_final_df)\n",
    "\n",
    "    print(f\"Fold {fold+1} Pair-Level Accuracy: {fold_accuracy:.4f}\")\n",
    "    print(f\"Fold {fold+1} Pair-Level f1: {fold_f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14c412",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c703df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cross-Validation Summary\n",
      "Average Pair-Level Accuracy: 0.7971\n",
      "Std. Dev: 0.0945\n",
      "Average Pair-Level f1: 0.7974\n",
      "Std. Dev: 0.0837\n",
      "\n",
      "Detailed CV results saved to: /Users/photoli93/Desktop/Projets perso Python/esa_fake_or_real/results/baseline_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine all folds’ results\n",
    "cv_results_df = pd.concat(fold_results, ignore_index=True)\n",
    "\n",
    "# Summary\n",
    "mean_acc = np.mean(pair_accuracies)\n",
    "std_acc = np.std(pair_accuracies)\n",
    "mean_f1 = np.mean(pair_f1)\n",
    "std_f1 = np.std(pair_f1)\n",
    "\n",
    "print('-' * 80)\n",
    "print(\"\\nCross-Validation Summary\")\n",
    "print(f\"Average Pair-Level Accuracy: {mean_acc:.4f}\")\n",
    "print(f\"Std. Dev: {std_acc:.4f}\")\n",
    "print(f\"Average Pair-Level f1: {mean_f1:.4f}\")\n",
    "print(f\"Std. Dev: {std_f1:.4f}\")\n",
    "\n",
    "# Save detailed results\n",
    "output_path = config.OUTPUT_DIR / \"baseline_cv_results.csv\"\n",
    "cv_results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nDetailed CV results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c3171",
   "metadata": {},
   "source": [
    "DistilBERT is clearly superior in all core metrics (accuracy & F1-score) but the baseline model is faster and lightweight, suitable for quick experiments or resource-constrained environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376077c",
   "metadata": {},
   "source": [
    "# End of baseline evaluation notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esa_fake_or_real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
